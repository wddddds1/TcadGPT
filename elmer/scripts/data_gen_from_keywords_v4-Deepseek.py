import os
import json
import time
import openai
from concurrent.futures import ThreadPoolExecutor, as_completed

# åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
# client = openai.Client(api_key='sk-REDACTED', base_url="https://api.deepseek.com")
client = openai.Client(api_key='sk-REDACTED', base_url="https://api.deepseek.com")


input_folder = "data/sources/elmer/keyword_pair"
output_folder = "data/sources/elmer/alpaca_output"
os.makedirs(output_folder, exist_ok=True)


def build_prompt(paragraph, keyword):
    return (
        """
        ä½ çš„ä»»åŠ¡æ˜¯ï¼šæ ¹æ®æä¾›çš„ä¸“ä¸šæ®µè½ä¸å…³é”®è¯ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ä¸­æ–‡é—®ç­”æ•°æ®ï¼Œç”¨äºå¾®è°ƒ Alpaca æ ¼å¼æ¨¡å‹ã€‚
        
        è¾“å‡ºå¿…é¡»ä¸º JSON æ•°ç»„ï¼Œæ¯æ¡æ•°æ®ç»“æ„å¦‚ä¸‹ï¼š
        {
          "instruction": "ç”¨æˆ·é—®é¢˜",
          "input": "",
          "output": "ç³»ç»Ÿå›ç­”"
        }
        
        è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„èŒƒï¼š
        
        1. åªç”Ÿæˆä¸­æ–‡å†…å®¹ï¼Œä¸å¾—å‡ºç°è‹±æ–‡ã€‚
        
        2. æ®µè½å¯èƒ½ä¸ºç›®å½•ã€å‚è€ƒæ–‡çŒ®ç­‰éæŠ€æœ¯å†…å®¹ï¼Œè‹¥ä¸åŒ…å«å®Œæ•´è¯­ä¹‰å¥æˆ–å…³é”®è¯ç›¸å…³ä¿¡æ¯ï¼ˆéä»…æåŠå…³é”®è¯ï¼‰ï¼Œè¯·è·³è¿‡è¯¥å…³é”®è¯ï¼Œè¾“å‡ºç©ºæ•°ç»„ï¼š[]ã€‚
           åŒæ—¶ï¼Œå¦‚æœå…³é”®è¯å¹¶éåˆç†çš„æŠ€æœ¯å…³é”®è¯ï¼Œä¹Ÿè¯·ç›´æ¥è·³è¿‡è¯¥å…³é”®è¯ã€‚
        
        3. é—®é¢˜ä¸å›ç­”åº”æ ¹æ®å…³é”®è¯ç±»å‹é‡‡ç”¨ä¸åŒé£æ ¼ï¼š
           - **æ¦‚å¿µ/ç‰©ç†åŸç†ç±»**ï¼šå®šä¹‰ã€æœºåˆ¶ã€åŸç†ã€ç›¸å…³æ¨¡å‹ç­‰ã€‚
           - **å…¬å¼/å˜é‡ç±»**ï¼šç‰©ç†æ„ä¹‰ã€è®¡ç®—æ–¹æ³•ã€å•ä½ã€å»ºæ¨¡æ–¹å¼ç­‰ã€‚
           - **ä»¿çœŸæŒ‡ä»¤/å‘½ä»¤ç±»**ï¼šç”¨é€”ã€æ ¼å¼ã€å‚æ•°ã€å…¸å‹ç”¨æ³•ã€ç»„åˆæ–¹å¼ç­‰ã€‚
           - **è½¯ä»¶æ¨¡å—ç±»**ï¼šåŠŸèƒ½ã€ä½œç”¨ã€å¯åŠ¨æ–¹å¼ã€ä¸å…¶ä»–æ¨¡å—çš„å…³ç³»ç­‰ã€‚
           - **è¾“å…¥å‚æ•°/é…ç½®é¡¹ç±»**ï¼šä½œç”¨ã€ä½¿ç”¨æ–¹å¼ã€æœ‰å“ªäº›å¯é€‰å‚æ•°ã€ä»¿çœŸç”¨é€”ã€ä»¿çœŸå½±å“ã€å•ä½ã€å–å€¼èŒƒå›´ã€é»˜è®¤å€¼ç­‰ã€‚
        
        4. æ¯ä¸ªå…³é”®è¯åº”å°½é‡è¦†ç›–å…¶åœ¨æ®µè½ä¸­æ¶‰åŠçš„å…¨éƒ¨çŸ¥è¯†ç‚¹ï¼Œä»å¤šä¸ªè§’åº¦æé—®ï¼Œç¡®ä¿é—®ç­”å†…å®¹ä¸é‡å¤ã€ä¸ç­‰ä»·è¡¨è¾¾ã€‚
        
        5. å›ç­”åº”å‡†ç¡®ã€æ¸…æ™°ã€æœ‰æ“ä½œæ€§ï¼Œå¯åŒ…å«å‘½ä»¤ã€å…¬å¼ç­‰ä¸“ä¸šä¿¡æ¯ã€‚æŒ‡ä»¤å’Œå‚æ•°ç±»é—®é¢˜å¯ç®€æ´æ˜äº†ä½†éœ€è¦ç»™ä¸€ä¸ªä¾‹å­ï¼Œæ¦‚å¿µå®šä¹‰å…¬å¼ç±»éœ€è¦è¯¦ç»†æ·±å…¥çš„è§£é‡Šã€‚
        
        6. ç¤ºä¾‹æé—®é£æ ¼ï¼ˆå¯å‚è€ƒï¼Œä¸å±€é™ï¼‰ï¼š
           - åœ¨ ElmerSolver ä¸­ï¼Œå¦‚ä½•è®¾ç½®çº¿æ€§ç³»ç»Ÿçš„ GMRES é‡å¯å‚æ•°ï¼Ÿ
           - éçº¿æ€§ Gaussâ€“Seidel è¿­ä»£çš„ä¸»è¦ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
           - åœ¨ sif æ–‡ä»¶ä¸­ï¼Œå¦‚ä½•æŒ‡å®šç½‘æ ¼æ•°æ®åº“ç›®å½•ï¼Ÿ
           - Navier-Stokes æ±‚è§£å™¨çš„ Flow Model é€‰é¡¹æ”¯æŒå“ªäº›å–å€¼ï¼Ÿ
        
        7. è‹¥æ®µè½ä¸­å…³é”®è¯ä¿¡æ¯é‡å¤å‡ºç°ï¼Œè¯·ä¸ºæ¯ä¸ªç”¨é€”åˆ†åˆ«ç”Ÿæˆé—®é¢˜ï¼›åŒä¸€å…³é”®è¯ç”Ÿæˆå¤šä¸ªé—®é¢˜æ—¶ï¼Œç¦æ­¢ä»…æ”¹å†™è¯­åºæˆ–è¯æ±‡ã€‚
        
        8. è¾“å‡ºå¿…é¡»ä¸ºåˆæ³•çš„ JSON æ•°ç»„ï¼Œä»…è¿”å›æ•°æ®æœ¬èº«ï¼Œä¸å¾—åŒ…å« Markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚```jsonï¼‰ã€‚
    
        9. å½“å…³é”®è¯æ˜¯æŒ‡ä»¤æˆ–å‚æ•°æ—¶ï¼Œé—®é¢˜ä¸­å°½é‡**ä¸è¦ç›´æ¥å‡ºç°æŒ‡ä»¤å**ï¼Œåº”é€šè¿‡åŠŸèƒ½æè¿°æ¨¡ä»¿ç”¨æˆ·æé—®ï¼Œå›ç­”ä¸­å†æ¸…æ¥šæŒ‡å‡ºæŒ‡ä»¤åä¸ç¤ºä¾‹ã€‚
            æ¯”å¦‚å½“å…³é”®è¯æ˜¯ BDF Order æ—¶ï¼Œä¸è¦é—®ï¼š
            "åœ¨ sif æ–‡ä»¶ä¸­ï¼ŒBDF Order çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"ç„¶åå›ç­”"BDF Order ç”¨äºæŒ‡å®šæ—¶é—´ç¦»æ•£é˜¶æ•°..."
            è€Œè¦é—®:"åœ¨ sif æ–‡ä»¶ä¸­ç”¨äºæŒ‡å®šæ—¶é—´ç¦»æ•£é˜¶æ•°çš„å…³é”®å­—æ˜¯ä»€ä¹ˆï¼Ÿ"ç„¶åå›ç­”"å…³é”®å­—æ˜¯ BDF Orderï¼Œç”¨æ³•ä¸º...æ¯”å¦‚..."
            
        10. **ä¸¥ç¦è¾“å‡ºä¸æ®µè½å†…å®¹æ— å…³æˆ–ä¸å¯éªŒè¯çš„ä¿¡æ¯**ï¼Œå°¤å…¶ä¸å¾—ç”Ÿæˆï¼š
            - â€œæ ¹æ®ä¸Šè¿°æ®µè½â€ã€â€œè¯¥ä¾‹ä¸­â€ã€â€œå¦‚å›¾æ‰€ç¤ºâ€ç­‰å†…å®¹ï¼›
            - å¾®è°ƒè®­ç»ƒä¸­æ— æ³•è·å–æ®µè½ä¸Šä¸‹æ–‡ï¼Œå› æ­¤ç¦æ­¢å‡ºç°å¼•ç”¨æ®µè½çš„è¯­è¨€ã€‚
            
        11. å½“å…³é”®è¯æ˜¯æŒ‡ä»¤æˆ–å‚æ•°æ—¶ï¼Œç”Ÿæˆçš„é—®ç­”å¯¹ä¸­å¿…é¡»è¦åŒ…å«ä¸€ä¸ªå¦‚ä¸‹å½¢å¼çš„é—®é¢˜: 
            "åœ¨(æ±‚è§£æ­¥éª¤ï¼Œå¦‚ ElmerSolver)ä¸­ï¼Œ(å…·ä½“åŠŸèƒ½çš„æè¿°)çš„æŒ‡ä»¤æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼Œå›ç­”"(xxåŠŸèƒ½)çš„æŒ‡ä»¤æ˜¯ï¼Œç”¨æ³•ä¸ºxxï¼Œæ¯”å¦‚ï¼ˆä¸¾ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚ï¼‰"
            
        12. å½“ç”Ÿæˆå…¬å¼æ—¶ï¼Œè¯·ç¡®ä¿å…¶èƒ½æ­£ç¡®æ¸²æŸ“ï¼Œè¯·æ­£ç¡®ä½¿ç”¨$æˆ–$$æ¥åŒ…è£¹ï¼Œåœ¨ä¸€æ®µæ–‡æœ¬ä¸­çš„å…¬å¼è¯·ç”¨$æ¥åŒ…è£¹ã€‚
        
        13. åŠ¡å¿…æ³¨æ„ä¸è¦åœ¨å¼€å¤´æˆ–ç»“å°¾æ·»åŠ ```jsonå’Œ```ã€‚
            
        """
        f"""
        æ®µè½å†…å®¹å¦‚ä¸‹ï¼š
        {paragraph}

        å…³é”®è¯ï¼š
        {keyword}
        """
    )


def call_api(paragraph, keyword, index):
    content = build_prompt(paragraph, keyword)
    for retry in range(2):
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé«˜è´¨é‡æ•°æ®ç”ŸæˆåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": content}
                ]
            )
            result = response.choices[0].message.content.strip()
            return result
        except Exception as e:
            print(f"[Thread-{index}] è¯·æ±‚å¤±è´¥ï¼Œé‡è¯• {retry + 1}/5ï¼š{e}")
            time.sleep(2 ** retry)
    return None


def process_jsonl_file(file_path, output_path, stats):
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”éç©º
    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
        print(f"â© è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    total_keywords = 0
    processed_keywords = 0
    output = []
    new_data_count = 0
    start_time = time.time()

    tasks = []
    with ThreadPoolExecutor(max_workers=1000) as executor:
        for line in lines:
            data = json.loads(line)
            text = data["text"]
            keywords = data.get("keywords", [])
            total_keywords += len(keywords)
            for i, kw in enumerate(keywords):
                tasks.append(executor.submit(call_api, text, kw, i))

        for future in as_completed(tasks):
            result = future.result()
            if result:
                try:
                    blocks = json.loads(result)
                    if isinstance(blocks, list):
                        output.extend(blocks)
                        added_count = len(blocks)
                    else:
                        output.append(blocks)
                        added_count = 1

                    new_data_count += added_count
                    if new_data_count % 500 == 0:
                        print(f"âœ… å·²æ–°å¢ {new_data_count} æ¡é—®ç­”ï¼ˆå½“å‰æ–‡æ¡£ç´¯è®¡ï¼š{len(output)}ï¼‰")

                except Exception as e:
                    print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼š{e}ï¼ŒåŸå§‹å†…å®¹ï¼š{result}")

            processed_keywords += 1
            stats["total_done"] += 1

            elapsed = time.time() - start_time
            avg_time = elapsed / processed_keywords if processed_keywords else 0
            remain_current = avg_time * (total_keywords - processed_keywords)
            remain_total = avg_time * (stats["total_all"] - stats["total_done"])

            print(f"[{os.path.basename(file_path)}] å½“å‰: {processed_keywords}/{total_keywords}ï¼Œ"
                  f"å…¨å±€: {stats['total_done']}/{stats['total_all']}ï¼Œå¹³å‡: {avg_time:.1f}sï¼Œ"
                  f"å‰©ä½™: {remain_current:.1f}sï¼ˆå½“å‰ï¼‰ï¼Œ{remain_total:.1f}sï¼ˆæ€»ï¼‰")

    with open(output_path, "w", encoding="utf-8") as f_out:
        for item in output:
            f_out.write(json.dumps(item, ensure_ascii=False) + "\n")


if __name__ == "__main__":
    stats = {"total_all": 0, "total_done": 0}
    jsonl_files = [f for f in os.listdir(input_folder) if f.endswith(".jsonl")]

    # é¦–å…ˆç»Ÿè®¡æ€»å…³é”®è¯æ•°
    for file in jsonl_files:
        with open(os.path.join(input_folder, file), "r", encoding="utf-8") as f:
            for line in f:
                try:
                    stats["total_all"] += len(json.loads(line).get("keywords", []))
                except:
                    continue

    print(f"\nğŸ“Š æ€»å…³é”®è¯æ•°ï¼š{stats['total_all']}ï¼Œæ–‡ä»¶æ•°ï¼š{len(jsonl_files)}")

    for filename in jsonl_files:
        input_path = os.path.join(input_folder, filename)
        output_filename = filename.replace(".jsonl", "_alpaca.jsonl")
        output_path = os.path.join(output_folder, output_filename)

        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”éç©ºï¼Œåˆ™è·³è¿‡å¤„ç†
            print(f"\nâ© è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {filename}")

            # æ›´æ–°å·²å¤„ç†çš„å…³é”®è¯ç»Ÿè®¡
            with open(input_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        stats["total_done"] += len(json.loads(line).get("keywords", []))
                    except:
                        continue
            continue

        print(f"\nğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {filename}")
        process_jsonl_file(input_path, output_path, stats)

    print("\nâœ… æ‰€æœ‰å¤„ç†å®Œæˆï¼")
